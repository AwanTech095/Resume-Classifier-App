# -*- coding: utf-8 -*-
"""NLP_Project1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eqb_WAKSK7HjAn49SdkPVMQXs-jhMt6R

# **Resume Classification Pipeline**
Automatically categorize resumes by domain using NLP techniques.

#**1. Data Loading & Inspection**
Goal: Load the raw dataset containing resumes and their labeled categories (e.g., "Data Science," "HR").

Key Insight: Understanding the data structure (e.g., number of resumes, category distribution) ensures appropriate preprocessing.
"""

import pandas as pd
import numpy as np

data = pd.read_csv('/content/UpdatedResumeDataSet.csv')
print(data.shape)
print(data)

"""#2. **Text Preprocessing**
**2.1 Normalization**

**Lowercasing:** Standardize all text to lowercase to avoid treating "Python" and "python" as different words.

**HTML/URL Removal:** Strip HTML tags and hyperlinks, which add noise to the text.

**Line Break Handling:** Replace newlines and tabs with spaces for uniform formatting.

#**2.2 Noise Reduction**
**ASCII-Only Text:** Remove non-English characters (e.g., é, ü) to focus on standard text.

**Smart Punctuation:** Preserve critical symbols (e.g., "+" for "C++") while discarding irrelevant punctuation.

**Stopword Removal:** Filter out common words (e.g., "the," "and") that lack predictive power.

#**Lowercasing**
"""

data['Resume'] = data['Resume'].str.lower()
print(data)

"""#**HTML Tags Removal**"""

import re
def remove_html_tags(text):
  pattern=re.compile('<.*?>')
  return pattern.sub(r'',text)

data['Resume']=data['Resume'].apply(remove_html_tags)
print(data)

"""#**Removal of URLs**"""

def remove_url(text):
  pattern=re.compile(r'https?://\S+|www\.\S+')
  return pattern.sub(r'',text)

data['Resume']=data['Resume'].apply(remove_url)
print(data)

"""#**Removal of Line Breaks**"""

import re

def remove_line_breaks(text):
    return re.sub(r'[\r\n]+', ' ', text)

data['Resume'] = data['Resume'].apply(remove_line_breaks)
print(data)

"""#**Removal of ASCII Formatted Text**"""

def remove_non_ascii(text):
    return text.encode("ascii", "ignore").decode()

data['Resume'] = data['Resume'].apply(remove_non_ascii)
print(data)

"""#**Removal of Punctuation**"""

import string
import re

# Keep only + and . — remove rest of punctuation(for better classification of words like C++, Node.js etc)
def smart_punct_removal(text):
    allowed = ['+', '.']
    to_remove = ''.join(ch for ch in string.punctuation if ch not in allowed)
    return text.translate(str.maketrans('', '', to_remove))

data['Resume'] = data['Resume'].apply(smart_punct_removal)
print(data)

"""#**Stopwords Removal**"""

from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def remove_stopwords(text):
    return " ".join([word for word in text.split() if word.lower() not in stop_words])

data['Resume'] = data['Resume'].apply(remove_stopwords)
print(data)

"""#**3. Linguistic Processing**
**3.1 Tokenization**

Split resumes into individual words/tokens while ignoring grammatical structures (e.g., parsing only words, not sentences).

**3.2 Lemmatization**

Reduce words to their base forms (e.g., "running" → "run") to consolidate similar terms and reduce feature space.

**3.3 Final Clean Text**

Rejoin processed tokens into coherent strings for feature extraction, ensuring the text is modeling-ready.

**4. Feature Engineering (TF-IDF)**

TF-IDF Vectorization: Convert text into numerical features by weighing terms based on:

**Term Frequency (TF):** How often a word appears in a resume.

**Inverse Document Frequency (IDF):** How rare the word is across all resumes.

Why TF-IDF? It highlights discriminative terms (e.g., "TensorFlow" in Data Science resumes) while downplaying filler words.

#**Tokenization**
"""

import spacy

#load spaCy English model without unnecessary components for speed
nlp = spacy.load("en_core_web_sm", disable=["parser", "tagger", "ner"]) #activated only the tokenizer
texts = data['Resume'].tolist()

#Tokenize in batches using spaCy pipe(fastest way)
docs = list(nlp.pipe(texts, batch_size=1000))

data['tokens'] = [[token.text for token in doc] for doc in docs]
print(data)

"""#**Lemmatization**"""

import nltk
from nltk.stem import WordNetLemmatizer

nltk.download('wordnet')
nltk.download('omw-1.4')        #optional: extra lemma mappings
lemmatizer = WordNetLemmatizer()

def lemmatize_tokens(tokens):
    return [lemmatizer.lemmatize(token) for token in tokens]

data['lemmatized_tokens'] = data['tokens'].apply(lemmatize_tokens)
print(data)

"""#**Saving the Cleaned Resume Dataset**"""

#convert list of tokens back to cleaned string for TF-IDF/BOW
data['Cleaned_Text'] = data['lemmatized_tokens'].apply(lambda tokens: ' '.join(tokens))
final_data = data[['Category', 'Cleaned_Text']]
final_data.to_csv('Cleaned_Resume_Dataset.csv', index=False)

"""#**Feature Engineering (TF-IDF)**"""

from sklearn.feature_extraction.text import TfidfVectorizer

#initialize TF-IDF with n-gram support (unigrams + bigrams)-->makes it more efficient
vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=2, max_features=10000)
X = vectorizer.fit_transform(final_data['Cleaned_Text'])
y = final_data['Category']

"""#**5. Model Training & Evaluation**
**5.1 Data Splitting**

Divide the dataset into training (80%) and testing (20%) sets, preserving the original category proportions.

**5.2 Logistic Regression**

Train a simple yet interpretable classifier to predict resume categories from TF-IDF features.

**5.3 Performance Metrics**

**Accuracy:** Overall correctness of predictions.
**Precision/Recall:** Per-category performance to identify model strengths (e.g., excels at "Engineering" but struggles with "HR").

#**Modelling**
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

#split data into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

#initialise the model
model = LogisticRegression(max_iter=1000)  #increase max_iter to ensure convergence
model.fit(X_train, y_train)

#predict on test set
y_pred = model.predict(X_test)

"""#**Evaluation/Performance**"""

#classification performance
print("Accuracy:",accuracy_score(y_test,y_pred))
print("\nClassification Report:\n",classification_report(y_test,y_pred))
print("\nConfusion Matrix:\n",confusion_matrix(y_test,y_pred))
#precision,recall,f1-score,support for each category

"""#**Actual Vs Predicted**"""

import pandas as pd

results = pd.DataFrame({
    'Original': y_test,
    'Predicted': y_pred
})
print(results.sample(10))

import joblib

# Save model and vectorizer
joblib.dump(model, 'resume_classifier.pkl')
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')